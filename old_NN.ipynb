{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYsF3ESMTr17YXELCUZMOo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9Jf7KeEfKux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76da2c08-5e4c-409d-ee2b-1261d1cc5e28",
        "collapsed": true
      },
      "source": [
        "!pip install ucimlrepo #for specific dataset\n",
        "import numpy as np\n",
        "import random"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#assume that you're dealing with a 28x28 pixel image\n",
        "#try using this as your dataset: https://www.nist.gov/srd/nist-special-database-19\n",
        "#just code the skeleton first and then handle how you'll deal with the data.\n",
        "#might have to work with letters instead of numbers since lecun took down the mnist dataset with handwritten digits... :(\n",
        "\n",
        "#make sure you use classes and create test functions which test the code\n",
        "\n",
        "#step 1: code the skeleton and see if it spits out random numbers correctly. this part is super-duper easy; include an activation function (ReLU or sigmoid squishification)\n",
        "\n",
        "#step 2: implement gradient descent via backpropagation\n",
        "\n",
        "#step 3: figure out how to interface data with the algorithm\n",
        "\n",
        "#step 4: train the network by dividing data up into mini-batches (stochastic gradient descent)\n",
        "\n",
        "#ok so coded the training function. next step is conditioning the data in the format that i specified in the training function\n",
        "\n",
        "\n",
        "#training on the iris data set for simplicity.\n",
        "#Fisher, R. (1936). Iris [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C56C76.\n"
      ],
      "metadata": {
        "id": "SbiyHEklfOn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myNN():\n",
        "  def __init__(self, cfg):\n",
        "      self.cfg = cfg\n",
        "      self.hidden = len(cfg) - 2 #columns in the cfg row vector - 2, which gives the number of hidden layers\n",
        "      self.input_dim = cfg[0]\n",
        "      self.output_dim = cfg[-1]\n",
        "      self.weight = [np.random.rand(y,x) for x,y in zip(cfg[:-1], cfg[1:])] #(y,x) since we want the first input to be the number of columns (inputs) and second input to the number of rows (outputs)\n",
        "      self.bias = [np.random.rand(x,1) for x in cfg[1:]] #for now, each node has a bias, but i may modify that later; can use conditionals to modify\n",
        "\n",
        "  def mod_weight(self, m: list, p: float):\n",
        "    if not isinstance(m, list):\n",
        "        raise TypeError(\"m must be a list!\")\n",
        "    elif not isinstance(p, float):\n",
        "        raise TypeError(\"p must be a float!\")\n",
        "    else:\n",
        "        self.weight = [self.weight[i]+p*x for i, x in enumerate(m)]\n",
        "  def mod_bias(self, b: list, p: float):\n",
        "    if not isinstance(b, list):\n",
        "        raise TypeError(\"b must be a list!\")\n",
        "    elif not isinstance(p, float):\n",
        "        raise TypeError(\"p must be a float!\")\n",
        "    else:\n",
        "        self.bias = [self.bias[i]+p*x for i, x in enumerate(b)]\n",
        "\n",
        "  def activations(self, activ: np.array): #given input activations, computes the sigmoid/dsigmoid function at each node (other than the input neurons of course)\n",
        "    if activ.size != self.input_dim:\n",
        "      raise ValueError(f\"inconsistent input dimension! got {activ.size}, expected {(self.input_dim,1)}\")\n",
        "    else:\n",
        "      def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "      def dsigmoid(x): #derivative of the sigmoid function\n",
        "        return np.exp(-x) / (1 + np.exp(-x))**2\n",
        "      firing = [activ] #activations - beginning with the input layer\n",
        "      change = [activ] #derivative of the the sigmoid function\n",
        "      for k in range(len(self.weight)): #excludes the first one because that's how we initiated\n",
        "        firing.append(sigmoid(self.weight[k] @ firing[k] + self.bias[k]))\n",
        "        change.append(dsigmoid(self.weight[k] @ firing[k] + self.bias[k]))\n",
        "      return firing, change\n",
        "\n",
        "  def backpropagate(self, activ: np.array, outcomes: np.array): #given activations and desired outcomes, computes the gradient of the cost function\n",
        "    if activ.size != self.input_dim:\n",
        "      raise ValueError(f\"inconsistent input dimension! got {(activ.size,1)}, expected {(self.input_dim,1)}\")\n",
        "    elif outcomes.size != self.output_dim:\n",
        "      raise ValueError(f\"inconsistent input dimension! got {(activ.size,1)}, expected {(self.output_dim,1)}\")\n",
        "    else:\n",
        "      firing, change = self.activations(activ)\n",
        "      #output layer\n",
        "      outer = 2*(firing[-1] - outcomes)\n",
        "      a0 = change[-1] #-1 index in change depends on the values in -2 of firing and so on...\n",
        "      f0 = firing[-2]\n",
        "      dC1 = outer*a0 #so this is a column vector\n",
        "      dC2 = outer*a0\n",
        "      dCw = []\n",
        "      dCb = []\n",
        "      #for loop including the input layer (ie, start with the layer closest to the output layer and then works backward)\n",
        "      for k in reversed(range(self.hidden + 1)):\n",
        "        #activations are rows; dC is columns\n",
        "        dCw.append(dC1 @ f0.T) #this feeds f0 from the previous pass\n",
        "        dCb.append(dC2)\n",
        "        a0 = change[k] #on the first pass, change[-2]; on the last pass, k=0. but this doesn't matter since it's not appended\n",
        "        f0 = firing[k-1] #on the first pass, firing[-3]\n",
        "        dC1 = (self.weight[k].T @ dC1)*a0 #since self.weight has a length which is one shorter than firing, k starts with the last element of weights\n",
        "        dC2 = sum(dC2)*a0\n",
        "      return dCw[::-1], dCb[::-1] #reversing since first entry corresponds to output\n",
        "\n",
        "  def new(self, activ: np.array, outcomes: np.array, dxw: float, dxb): #given some discretization step, outputs new weights and biases; importantly, takes the NEGATIVE of the gradient computed in the backpropagation method\n",
        "    if not isinstance(dxw, float):\n",
        "      raise TypeError(\"descritization dxm must be a float!\")\n",
        "    elif not isinstance(dxb, float):\n",
        "      raise TypeError(\"descritization dxb must be a float!\")\n",
        "    else:\n",
        "      w, b = self.backpropagate(activ, outcomes)\n",
        "      self.weight = [self.weight[i]-dxw*x for i,x in enumerate(w)]\n",
        "      self.bias = [self.bias[i]-dxb*x for i,x in enumerate(b)]\n",
        "\n",
        "  def run(self, activ: np.array, outcomes: np.array):  #given activations and outcomes, runs the neural network; a bit redundant, given that we have a whole activation function...\n",
        "    if outcomes.size != self.output_dim:\n",
        "      raise ValueError(f\"inconsistent input dimension! got {(outcomes.size,1)}, expected {(self.output_dim,1)}\")\n",
        "    else:\n",
        "      activation, _ = self.activations(activ)\n",
        "      return activation[-1], sum((activation[-1] - outcomes)**2) #gives the cost function output as well\n",
        "\n",
        "#batched training function\n",
        "#while loop which stops when the cost function is below some value and the change in the cost function is below some value\n",
        "#have a condition in the while loop to initialize it in new position if gradient is below the value but the cost function isn't below the other value\n",
        "#takes in data, batches it, takes the average of the negative gradients, modifies the weights and biases using the mod_weight and mod_bias methods in the neural network class\n",
        "\n",
        "def training(NN_structure, dataset, answer_key, batch_size, batch_number, C_min, dC_min, dxw, dxb):\n",
        "#answer_key: list of vectors, where each (column) vector corresponds to the expected values for one example\n",
        "#dataset: list of vectors, where each (column) vector holds the input activations for one example\n",
        "  network = myNN(NN_structure)\n",
        "  #function for testing the accuracy of the algorithm\n",
        "  def test_acc(): #how it works is that it takes the output vector, maps it to its nearest standard basis vector (using round now for simplicity), and then computes the ratio of number of correct answers to the total number of points in the dataset\n",
        "    L = len(dataset)\n",
        "    num_correct = 0 #number of correct answers\n",
        "    for k in range(L):\n",
        "      output,_ = network.run(dataset[k], answer_key[k]) #answer_key has no relevance here - it's used in the run method to compute the cost (whose output is muted here)\n",
        "      if (answer_key[k] == np.round(output)).all():\n",
        "        num_correct += 1\n",
        "      else:\n",
        "        continue\n",
        "    print(f\"accuracy: {num_correct*100/L}%\")\n",
        "    return num_correct*100/L\n",
        "  #running the test before training\n",
        "  before = test_acc()\n",
        "\n",
        "  #training\n",
        "  cost = C_min + 1\n",
        "  cost_change = dC_min + 1\n",
        "  i = 0\n",
        "  while cost > C_min:\n",
        "    avg_dCw = 0\n",
        "    avg_dCb = 0\n",
        "    for k in range(batch_size):\n",
        "      cost_change = cost\n",
        "      inputs = dataset[k+i]\n",
        "      desired_outputs = answer_key[k+i]\n",
        "      dCw, dCb = network.backpropagate(inputs, desired_outputs)\n",
        "      if isinstance(avg_dCw, int):\n",
        "        avg_dCw = dCw\n",
        "        avg_dCb = dCb\n",
        "      else:\n",
        "        avg_dCw = [avg_dCw[i]+x for i,x in enumerate(dCw)]\n",
        "        avg_dCb = [avg_dCb[i]+x for i,x in enumerate(dCb)]\n",
        "    avg_dCw = [x/batch_size for x in avg_dCw] #taking the average\n",
        "    avg_dCb = [x/batch_size for x in avg_dCb]\n",
        "    network.mod_weight(avg_dCw, -dxw) #taking negative since we need negative of the gradient\n",
        "    network.mod_bias(avg_dCb, -dxb)\n",
        "    _, cost = network.run(inputs, desired_outputs) #updating the cost\n",
        "    cost_change = abs(cost - cost_change) #ie, new_cost - old_cost\n",
        "    i += 1\n",
        "    if i > batch_number: #stops if we reached the max number of times we get to iterate\n",
        "      if cost_change > dC_min:\n",
        "        network = myNN(NN_structure) #ie, reinitializing. idk if this is a good idea or not lol\n",
        "        i = 0\n",
        "        avg_dCw = 0\n",
        "        avg_dCb = 0\n",
        "        cost_change = dC_min + 1\n",
        "        cost = C_min + 1\n",
        "        continue\n",
        "      else:\n",
        "        break\n",
        "    elif cost_change < dC_min:\n",
        "      if cost > C_min:\n",
        "        network = myNN(NN_structure) #ie, reinitializing. idk if this is a good idea or not lol\n",
        "        i = 0\n",
        "        avg_dCw = 0\n",
        "        avg_dCb = 0\n",
        "        cost_change = dC_min + 1\n",
        "        cost = C_min + 1\n",
        "        continue\n",
        "      else:\n",
        "        break\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "  print(f\"finished training after {i} batches\")\n",
        "\n",
        "  #running the test after training\n",
        "  after = test_acc()\n",
        "  return network, cost, cost_change, before, after\n",
        "\n",
        "#write a testing function which tests how well the algorithm does. may need to take everything into a class because i'll need to keep track of the first\n",
        "#instance of the neural network and compare it to the instance after it has been trained...\n",
        "#or i can just write it as part of the training and testing function... because that has one defined 'network' that we can work with"
      ],
      "metadata": {
        "id": "_I8jUED8frsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "implementing training of the specific iris dataset"
      ],
      "metadata": {
        "id": "ol9QReVJt8y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pulling the dataset\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "iris = fetch_ucirepo(id=53)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = iris.data.features #ie, these are the attributes based on which we will be determining the flower\n",
        "y = iris.data.targets #these are the answers\n",
        "\n",
        "# metadata\n",
        "#print(iris.metadata)\n",
        "\n",
        "# variable information\n",
        "#print(iris.variables)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BRjjrg67t7uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#conditioning the data\n",
        "data = X.to_numpy()\n",
        "data = [row.reshape(-1, 1) for row in data] #making every row a column\n",
        "num = 3\n",
        "vecs = np.eye(num) #something is up with forcing the elements of data and ans_to_vec, etc, to be column vectors...\n",
        "answers = y.to_numpy()\n",
        "ans_to_vec = [vecs[:,0].reshape(num,1) if x=='Iris-setosa' else vecs[:,1].reshape(num,1) if x=='Iris-versicolor' else vecs[:,2].reshape(num,1) for x in answers]\n",
        "\n",
        "#batching them together so that we can mix them (then separate them after)\n",
        "mixer = [(data[i],x) for i,x in enumerate(ans_to_vec)]\n",
        "random.shuffle(mixer)\n",
        "\n",
        "#now separating them (keeping the mixed set because it will be useful for later)\n",
        "dataset = [x[0] for x in mixer]\n",
        "answer_key = [x[1] for x in mixer]\n",
        "NN_structure = [dataset[0].size, 4, 4, answer_key[0].size]\n",
        "batch_size = 3\n",
        "batch_number = 30\n",
        "dxw = 0.7\n",
        "dxb = 0.7\n",
        "C_min = 0.1\n",
        "dC_min = 0.1\n",
        "\n",
        "#running the training function\n",
        "network, cost, cost_change, before, after = training(NN_structure, dataset, answer_key, batch_size, batch_number, C_min, dC_min, dxw, dxb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EldhfX5quqZt",
        "outputId": "5fa06719-3f55-417d-f2a9-9f6a34fadb7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.0%\n",
            "finished training after 5 batches\n",
            "accuracy: 33.333333333333336%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "82-JjTw4Tvja"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
